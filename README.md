# Real_Time_Transaction_Data_Processing_using_Databricks

In this project, we’ll explore a robust solution for handling real-time data streams using Azure Event Hub, Azure Databricks and the Medallion Architecture. We’ll walk through the entire process — from capturing JSON records to storing and refining them across different data layers.

This project revolves around capturing and processing streaming transaction data generated by a Python application. The key components and workflow involve:

Python Program: Generates JSON records every 3 seconds based on the above mentioned template.

Azure Event Hub: Acts as the messaging backbone to handle streaming data.

Azure Databricks: Acts as a transformation layer that follows Medallion Architecture which is a layered approach to data processing and storage.

Medallion Architecture: Organizes data into Bronze, Silver, and Gold layers that helps manage data quality and transformation.

Power BI: To visualize data and to get insights from it.

#Data Cleaning:

1.Remove irrelevant column (birthdate, bucket_start_date, bucket_end_date) & include necessary ones (enqueuedTime).

2.Remove duplicate data.

3.Fix structural errors (datatype format)

4.Inconsistent data format (change the format of enqueuedTime to 'yyyy-MM-dd HH:mm:ss')

#Business Logic:

1. For the products with "InvestmentStock" increase the limit to 5%

2. The customer name should be displayed in the template - <Lastname>(.)<space><Firstname>.  Here the lastname should be capitalized.

3. The domain of the customer’s email address should be changed from “@example.com” to “@outlook.com”.

4. In Transactions Limit the decimals of Price and Total fields to two places.

#Blog:

PART-1:  https://medium.com/@aravinde0797/real-time-transaction-data-processing-using-azure-databricks-part-1-eaf0d1b1df5a

PART-2:  https://medium.com/@aravinde0797/real-time-transaction-data-processing-using-azure-databricks-part-2-7a906fb7a93b
